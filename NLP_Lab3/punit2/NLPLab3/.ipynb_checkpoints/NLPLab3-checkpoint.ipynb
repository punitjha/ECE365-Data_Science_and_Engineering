{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and NetID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Punit K. Jha\"\n",
    "NetID = \"punit2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 3: Language Modeling\n",
    "=============\n",
    "In this problem set, your objective is to train a language model, evaluate it and explore how it can be used for language generation. Towards that end you will:\n",
    "\n",
    "- Train an n-gram language model.\n",
    "- Use that language model to generate representative sentences.\n",
    "- Study the effect of training data size, and language model complexity (n-gram size), on the modeling capacity of a language model.\n",
    "\n",
    "- **To submit this assignment, rename the whole directory as your NetID. Compress the whole directory using tar or zip, and submit ```Your_NetID.tgz``` or ```Your_NetID.zip``` on Compass.**\n",
    "\n",
    "Total points: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [nltk](https://www.nltk.org)\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python version\n",
      "python: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "nose: 1.3.7\n",
      "nltk: 3.4.5\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('nltk: {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training a language model\n",
    "\n",
    "Let us first train a 3-gram language model. We need a monolingual corpus, which we will get using nltk.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first extract from nltk's reuters corpus, 2 corpora in 2 different domains (here, subject areas), the food industry and the natural resources industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "food = ['barley', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copra-cake', 'grain', 'groundnut', 'groundnut-oil', 'potato', 'soy-meal', 'soy-oil', 'soybean', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'veg-oil', 'wheat']\n",
    "natural_resources = ['alum', 'fuel', 'gas', 'gold', 'iron-steel', 'lead', 'nat-gas', 'palladium', 'propane', 'tin', 'zinc']\n",
    "corpus = nltk.corpus.reuters\n",
    "food_corpus = corpus.raw(categories=food)\n",
    "natr_corpus = corpus.raw(categories=natural_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Your first task is to tokenize the raw text into a list of sentences, which are in turn a list of words. No need for any other kind of preprocessing such as lowercasing.\n",
    "\n",
    "- **Deliverable 1.1**: Complete the function `ece365lib.train.tokenize`. (5 points)\n",
    "- **Test**: `nose tests/test_train.py:test_d1_1_tk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ece365lib import train\n",
    "# nltk.download('punkt')\n",
    "reload(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk = train.tokenize_corpus(food_corpus)\n",
    "natr_corpus_tk = train.tokenize_corpus(natr_corpus)\n",
    "# food_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 5.015s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d1_1_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc8cad21230ac197e00a052d0ee39db2",
     "grade": true,
     "grade_id": "cell-8cd3d71a7ab015a1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Your second task is to pad your sentences with the start-of-sentence symbol `'<s>'` and end-of-sentence symbol `'</s>'`. These symbols are necessary to model the probability of words that usually start a sentence and those that usually end a sentence.\n",
    "\n",
    "- **Deliverable 1.2**: Complete the function `ece365lib.train.pad`. (5 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d1_2_pad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk_pd = train.pad_corpus(food_corpus_tk)\n",
    "natr_corpus_tk_pd = train.pad_corpus(natr_corpus_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "21\n",
      "['<s>', 'Australia', 'and', 'Canada', 'could', 'then', 'increase', 'their', 'wheat', 'exports', 'as', 'they', 'are', 'more', 'competitive', 'than', 'the', 'U.S.', ',', 'He', 'said', '.', '</s>']\n",
      "['Australia', 'and', 'Canada', 'could', 'then', 'increase', 'their', 'wheat', 'exports', 'as', 'they', 'are', 'more', 'competitive', 'than', 'the', 'U.S.', ',', 'He', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "print(len(food_corpus_tk_pd[45]))\n",
    "print(len(food_corpus_tk[45]))\n",
    "print(food_corpus_tk_pd[45])\n",
    "print(food_corpus_tk[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 5.189s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d1_2_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20586de90cbea259310cce679e7cac38",
     "grade": true,
     "grade_id": "cell-e450f77fb0dd8d38",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Your third task is to split the corpora into train, for training the language model, and test, for testing the language model. We will go with the traditional 80% (train), 20% (test) split. The first `floor(0.8*num_of_tokens)` should constitute the training corpus, and the rest should constitute the test corpus.\n",
    "\n",
    "- **Deliverable 1.3**: Complete the function `ece365lib.train.split_corpus`. (5 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d1_3_spc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tr, food_corpus_te = train.split_corpus(food_corpus_tk_pd)\n",
    "natr_corpus_tr, natr_corpus_te = train.split_corpus(natr_corpus_tk_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 5.195s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d1_3_spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11bea84771130de0f14203078b79f6b1",
     "grade": true,
     "grade_id": "cell-11d3bf643a00dd4c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into n-grams\n",
    "\n",
    "Your fourth task is to count n-grams in the text up to a specific order.\n",
    "\n",
    "- **Deliverable 1.4**: Complete the function `ece365lib.train.count_ngrams`. (20 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d1_4_cn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "[('It', 'also'), ('also', 'said'), ('said', 'that'), ('that', 'each'), ('each', 'year'), ('year', '1.575'), ('1.575', 'mln'), ('mln', 'tonnes'), ('tonnes', ','), (',', 'or'), ('or', '25'), ('25', 'pct'), ('pct', ','), (',', 'of'), ('of', 'China'), ('China', \"'s\"), (\"'s\", 'fruit'), ('fruit', 'output'), ('output', 'are'), ('are', 'left'), ('left', 'to'), ('to', 'rot'), ('rot', ','), (',', 'and'), ('and', '2.1'), ('2.1', 'mln'), ('mln', 'tonnes'), ('tonnes', ','), (',', 'or'), ('or', 'up'), ('up', 'to'), ('to', '30'), ('30', 'pct'), ('pct', ','), (',', 'of'), ('of', 'its'), ('its', 'vegetables'), ('vegetables', '.')]\n"
     ]
    }
   ],
   "source": [
    "reload(train);\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_ngrams, food_vocab = train.count_ngrams(food_corpus_tr, 3)\n",
    "natr_ngrams, natr_vocab = train.count_ngrams(natr_corpus_tr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 6.541s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d1_4_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d191851e6971e21af2ff6c83ee66757f",
     "grade": true,
     "grade_id": "cell-de09a842f96c1d10",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating n-gram probability\n",
    "\n",
    "Your last task in this part of the problem set is to estimate the n-gram probabilities p(w_i|w_{i-n+1}, w_{i-n+2}, .., w_{i-1}), with no smoothing. For the purposes of this exercise we will use the maximum likelihood estimate and perform no smoothing. \n",
    "\n",
    "- **Deliverable 1.5**: Complete the function `ece365lib.train.estimate`. (5 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d1_5_es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371778.0\n",
      "194\n",
      "one_g/N 0.0005218167831340209\n",
      "N_two 3960.0\n",
      "two_g 11\n",
      "two_g/N_two 0.002777777777777778\n",
      "N_three 8.0\n",
      "three_g 2\n",
      "three_g/N_three 0.25\n",
      "219782\n"
     ]
    }
   ],
   "source": [
    "reload(train);\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 2\n",
      "0.25\n",
      "2.0 1\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(train.estimate(food_ngrams, ['palm'], ['producer', 'of']))\n",
    "print(train.estimate(natr_ngrams, ['basis'], ['tested', 'the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 9.362s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d1_5_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dac46a5168b494b4b09d38eaa64d940",
     "grade": true,
     "grade_id": "cell-aedf095cf6b6ff3d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application: the speech recognition task takes human voice as its input and outputs text. If the pronunciation of two words are similar, Language Model can help decide which word to choose! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(type(food_ngrams))\n",
    "print(food_ngrams[('there', 'is', 'no')])\n",
    "print(food_ngrams[('their', 'is', 'no')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the count of 'there is no' and 'their is no', which word ('there' or 'their') is more likely to be taken as the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model is not only helpful in speech recogition, but text generation (*e.g.*, machine translation, summarization, image captioning), spelling correction and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "Now we will combine everything together and train our language model! One way to see what the language model has learned is to see the sentences it can generate.\n",
    "\n",
    "For the sake of simplicity, and for the purposes of later parts in this problem set, we use nltk's lm module to train a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "size_ngram = 3\n",
    "\n",
    "food_train, food_vocab = padded_everygram_pipeline(size_ngram, food_corpus_tk[:int(0.8*len(food_corpus_tk))])\n",
    "natr_train, natr_vocab = padded_everygram_pipeline(size_ngram, natr_corpus_tk[:int(0.8*len(natr_corpus_tk))])\n",
    "\n",
    "food_test = sum([['<s>'] + x + ['</s>'] for x in food_corpus_tk[int(0.8*len(food_corpus_tk)):]],[])\n",
    "natr_test = sum([['<s>'] + x + ['</s>'] for x in natr_corpus_tk[int(0.8*len(natr_corpus_tk)):]],[])\n",
    "\n",
    "food_lm = Laplace(size_ngram)\n",
    "natr_lm = Laplace(size_ngram)\n",
    "\n",
    "food_lm.fit(food_train, food_vocab)\n",
    "natr_lm.fit(natr_train, natr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask our language model to generate a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Coffee', 'does', 'not', 'know', 'how', 'much', 'they', 'planted', '.']\n",
      "['<s>', 'Currently', ',', 'there', 'was', 'a', '15.3', 'pct', 'increase', ',']\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "n_words = 10\n",
    "print(food_lm.generate(n_words, random_seed=3))  # random_seed makes the random sampling part of generation reproducible. \n",
    "print(natr_lm.generate(n_words, random_seed=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluating a language model\n",
    "\n",
    "Next, we evaluate our language models using the perplexity measure, and draw conclusions on how a change of domains (here, subject areas) can affect the performance of a language model. Perplexity measures the language model capacity at predicting sentences in a test corpus.\n",
    "\n",
    "Total: 20 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 2.1**: Complete the function `ece365lib.evaluate.get_perplexity`. (10 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d2_1_gp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ece365lib import evaluate\n",
    "reload(evaluate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8396.70521548381\n",
      "8546.64191689775\n",
      "5428.123517045601\n",
      "5490.520376861629\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "print(evaluate.get_perplexity(food_lm, food_test[:5000]))\n",
    "print(evaluate.get_perplexity(food_lm, natr_test[:5000]))\n",
    "print(evaluate.get_perplexity(natr_lm, natr_test[:5000]))\n",
    "print(evaluate.get_perplexity(natr_lm, food_test[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 119.337s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d2_1_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aedddf88a9ba13a57ab484e39a172f82",
     "grade": true,
     "grade_id": "cell-4a51f160883e4a82",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 2.2**: What observations can you make on the results? Is the domain shift affecting the performance of the language model? What are possible explanations? (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8b9fbf1566a81a7d25fc71cb362a632",
     "grade": true,
     "grade_id": "cell-75755ce98e18d29b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<b> We know that higher the conditional probability of the word sequence, the lower the perplexity. We see that as we the if we train on a corpus and use it to predict a entirely different test corpus then the perplexity increases i.e the test set probability decreases. So yes, the domain shift it affecting the perfomance of the language model. Since our model was trained on the \"food\" domain the model might not have even seen words in the \"natr\" domian so making predictions is difficult and has higher errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data size and model complexity\n",
    "\n",
    "Let us now see how the size of the training data and the complexity of the model we choose affects the quality of our language model.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we'd like to see the difference between a 2-gram model and a 3-gram model. Typically, with a larger n, the n-gram model gives us more information about the word sequence and has lower perplexity. \n",
    "\n",
    "For testing, we'll only be considering 5% instead of 20% of the test data for running time purposes. \n",
    "\n",
    "- **Deliverable 3.1**: Complete the function `ece365lib.train.vary_ngram`. (40 points)\n",
    "- **Test**: `nosetests tests/test_train.py:test_d3_1_vary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ece365lib import train\n",
    "reload(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 5393.67203181578, 3: 5424.76053819262}\n"
     ]
    }
   ],
   "source": [
    "n_gram_orders = [2, 3]\n",
    "\n",
    "train_corpus = natr_corpus_tk[:int(0.8*len(natr_corpus_tk))]\n",
    "test_corpus = natr_corpus_tk[int(0.8*len(natr_corpus_tk)): int(0.85*len(natr_corpus_tk))]\n",
    "\n",
    "results = train.vary_ngram(train_corpus, test_corpus, n_gram_orders)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 212.455s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_train.py:test_d3_1_vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6120859d56ef083aa7f4a3cd0df4436a",
     "grade": true,
     "grade_id": "cell-d94c934f22927e41",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we notice that the 3-gram language model actually performs worse than the 2-gram language model. This is due to the small size of the training corpus. A 3-gram language model is actually too complex of a model for a small training size. If our training data was larger, we would be seeing the opposite. If we trained 1-gram, 2-gram, and 3-gram models on 38 million words from the Wall Street Journal, we will get perplexity of 962, 170, 109 respectively on a test set of 1.5 million words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see a few examples of top frequent n-gram examples. Let's start with unigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',',), ('the',), ('<s>',), ('</s>',), ('.',), ('of',), ('to',), ('and',), ('said',), ('in',), ('a',), ('for',), ('The',), ('from',), ('pct',), ('mln',), ('at',), ('on',), (\"'s\",), ('is',)]\n"
     ]
    }
   ],
   "source": [
    "natr_ngrams, natr_vocab = train.count_ngrams(natr_corpus_tr, 3)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 1:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think unigram captures any grammatical information? How well do you think unigram captures the language information? \n",
    "\n",
    "<b>The unigram does really capture grammatical information as we can see that the comma,full stop, sentence start and stop, prepositions, conjunctions etc are the most frequent occuring words in the unigram model. However, the language information and sentence construction is poor as reflected from higher perplexity scores as compared to bi,tri-gram models.<b>\n",
    "\n",
    "Now let's see bigram and trigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', '</s>'), ('said', '.'), ('<s>', 'The'), ('in', 'the'), ('of', 'the'), ('&', 'lt'), ('lt', ';'), (',', 'the'), ('said', 'it'), ('said', 'the'), ('<s>', '``'), (',', \"''\"), (',', 'which'), ('to', 'the'), ('for', 'the'), (',', 'a'), ('on', 'the'), (',', 'and'), ('mln', 'dlrs'), ('<s>', 'It')]\n",
      "[('said', '.', '</s>'), ('&', 'lt', ';'), ('.', \"''\", '</s>'), ('<s>', 'The', 'company'), ('<s>', 'It', 'said'), ('he', 'said', '.'), ('ounces', 'of', 'gold'), ('year', '.', '</s>'), ('The', 'company', 'said'), ('...', '...', '...'), ('added', '.', '</s>'), ('oil', 'and', 'gas'), (',', 'it', 'said'), ('pct', '.', '</s>'), (',', \"''\", 'he'), (',', 'he', 'said'), ('it', 'said', '.'), ('sources', 'said', '.'), ('is', 'expected', 'to'), ('<s>', 'He', 'said')]\n"
     ]
    }
   ],
   "source": [
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 2:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 3:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with unigram, bigram and trigram can capture more information. \n",
    "Bigram language model can already capture some of the grammatical information, such as 'in the', 'of the'. However, the power of bigram is still limited. \n",
    "The trigram can output more adequate short phrases such as 'ounces of gold', 'The company said', 'oil and gas'. \n",
    "\n",
    "Therefore, typically the n-gram model with a larger n contains more information about the word sequence and thus, has lower perplexity. However, the tradeoff is the computational efficiency and memory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
